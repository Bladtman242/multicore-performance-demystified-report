\chapter{Experiments}
\label{chap:experiments}

In this chapter we look at multiple examples of multicore programs, focusing on
how they are affected by false sharing of cache lines. We start with a few
slightly contrived programs in section \ref{sec:contrived}, to show the extremes
of false sharing overhead. We then move on to more practical examples such as
histogram building, sorting, and k-means clustering in section
\ref{sec:practical}

Unless otherwise noted, experiments are run with 4, 8, and 48 threads on the i5,
i7, and Xeon platforms, respectively. These numbers were chosen based on the
experiments in the following section.

\section{Platform experiments}
Before we get into our multicore performance experiments, it is worth verifying
some assumptions about the hardware platforms we perform them on. In the
following sections, we first assess noise in our experiments by examining the CPU
load when not running experiments. We then gauge our platforms' effective or
perceived degree of parallelism, by measuring how well simple tasks scale with the
number of threads used. We finally measure the access times of the different
levels of the cache hierarchy.

\subsection{CPU idle load}
Our experiments do not run on bare metal: The underlying operating system and
other user-applications may put load on the CPU, and skew our results.
To assess the amplitude of this noise, we measure the CPU load on the platforms
when not running experiments.
The CPU load information is gathered from the Linux top utility on the
i5 and i7 platforms, and from the Windows typeperf command on the Xeon
platform.

Values from both tools are sampled over 1-second intervals. The top
utility gives integer values, while typeperf gives decimal values.

The average values for the durations of the experiments are: 0.20\% for the i5
platform, 0.02\% for the i7 platform, and 0.09\% for the Xeon platform.

Figure \ref{fig:cpuload} shows the CPU load of all three platforms when no
experiments are running.

The values are given as percentages of a load that would keep all cores busy
100\% of the time. Therefore, the Xeon platform is actually the busiest, despite
not having the highest average. Expressing the load averages as percentages of a
load that would keep a single, virtual core busy, the loads are 0.79\%, 0.16\%,
and 4.14\% for the i5, i7, and Xeon platform respectively.

For the performance effects we wish to examine in the remainder of this report,
this level of noise seems to be acceptable.

\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{1\textwidth}
		\input{plots/cpuloadx1.tex}
		\caption{i5}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/cpuload-desktop.tex}
		\caption{i7}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/cpuload-server.tex}
		\caption{Xeon}
	\end{subfigure}
	\caption{CPU baselines for benchmarks. The x-axes show CPU load in
	percent, as reported by the operating system (100\% load corresponds to
	all cores being busy 100\% of the time). The y-axes show time in
	minutes. Please note that both axes differ between the plots.}
	\label{fig:cpuload}
\end{figure}

\subsection{Degrees of parallelism}
It is not entirely clear how much work our platforms can do in parallel. We know
how many physical cores they each have, and we know how many and virtual cores
they each have with hyper-threading. But hyper-threading is not the same as
having additional physical cores: Virtual cores share hardware such as caches,
do not fully execute two threads at once. Rather, hyper-threading allows two
threads to interleave on a single physical core, so that when one thread is
waiting for IO, the other may perform CPU bound work, and vice-versa.

To find the effective or perceived degree of parallelism our platforms support,
we run four experiments with different load types: IO bound, CPU bound, and mixed
load. 

Each experiment runs a fixed number of iterations for each of the platforms.
Work load is divided between evenly threads, so more threads means fewer
iterations per thread.

The experiments are:

\begin{description}
	\item [mem-reads] A memory bound experiment. Each thread reads elements
		from an array and keeping a running sum on the stack. The array
		is traversed in random order, and contains random integers. The
		array size length is chosen as twice the L3 cache size for the
		given platform. The code for this experiment is shown in code
		snippet \ref{code:sum}. The loop runs for 40E6, 32E6, and 64E6
		iterations on the i5, i7, and Xeon platforms respectively.
	\item [mem-writes] The exact same as mem-reads, but the sum is written
		back to the array in each iteration.
		The loop runs for 30E6, 32E6, and 64E6 iterations on the i5, i7,
		and Xeon platforms respectively.
	\item [CPU] The CPU bound experiment, shown in code snippet
		\ref{code:div}. Each thread performs a sequence of
		floating-point divisions.
		The divisor is a value greater than 1, chosen such that \java{d}
		never becomes less than or equal to 1.
		The loop runs for 40E6, 108E6, and 750E6 iterations on the i5,
		i7, and Xeon platforms respectively.
	\item [mixed] A mixed-load experiment. This experiment uses half of the
		threads to run the mem-reads experiment, and the other half to
		run the CPU experiment. Since the experiments divides work
		between the threads they are allotted, the mixed experiment
		performs twice as much work per thread as the other experiments.
\end{description}

Figures \ref{fig:parallel-i5}, \ref{fig:parallel-i7}, and
\ref{fig:parallel-xeon}, show the wall-clock times of the four experiments on
the i5, i7, and Xeon platforms respectively.

The mixed experiment parallelises the best (remember that it divides its threads
between the mem-reads and CPU experiments, which means that the mixed experiment
for 2 threads performs as much work as the mem-reads and CPU experiments do
together for 1 thread, and so forth). This agrees with our intuition of
hyper-threading works.

Across the different types of loads, we see that 4, 8, and 48 threads are good
thread counts for the i5, i7, and Xeon platforms respectively.

\begin{code}
\begin{Verbatim}[frame=single]
  let taskCount parallel tasks do {
    //start indexes are evenly spaced for the threads
    final int startIndex = ...;
      double sum = 0.0;
      int k = startIndex;
      for (int j=0; j < workPerThread; j++){
        if(k<arraySize - 1){
          ++k;
        } else {
          k=0;
        }
        sum += array[indices[k]];
      }
  }
\end{Verbatim}
	\caption{Simplified code for the memory-bound parallelism experiment.
	The \java[indices] array contains all the indices of the \java{array}
	array in random order.}
	\label{code:sum}
\end{code}

\begin{code}
\begin{Verbatim}[frame=single]
  let taskCount parallel tasks do {
    double d = Double.MAX_VALUE;
    for (long j = 0; j < workPerThread; ++j) {
      d /= divisor;
    }
  }
\end{Verbatim}
	\caption{Simplified code for the CPU-bound parallelism experiment.}
	\label{code:div}
\end{code}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/paral.tex}
	\caption{Parallelism experiment on the i5 platform. The plot shows
	wall-clock execution time in ns., as a function of thread count.}
	\label{fig:parallel-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/paral-desktop.tex}
	\caption{Parallelism experiment on the i7 platform. The plot shows
	wall-clock execution time in ns., as a function of thread count.}
	\label{fig:parallel-i7}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/paral-server.tex}
	\caption{Parallelism experiment on the Xeon platform. The plot shows
	wall-clock execution time in ns., as a function of thread count.}
	\label{fig:parallel-xeon}
\end{figure}


\subsection{Memory hierarchy access-times}
Here we replicate the experiment used to measure read access times in
\cite{mystery}. Code snippet \ref{code:cyclic} shows the code for the core
measurement loop, figure \ref{fig:readtimes} shows the results. The experiment
performs a fixed number of memory read operations ($2^{25} =
33.54432\mathrm{E}6$) from an array. The access pattern is determined by the
contents of a pre-built array. The pattern is cyclic, so the number of read
operations and the working-set size are independent of each other, and
randomized to hinder cache prefetching. This allows us to measure the read times
of the different levels of the memory hierarchy, by choosing working set sizes
that are too large to fit in the smaller levels. If we e.g. wish to measure the
access times for L2 cache, we use a working set larger than the 32KiB that will
fit in L1, but no larger than the 256KiB that will fit in L2. The technique used
to fill the array, thereby deciding the access pattern, is rather intricate, so
rather than restating it here, I refer interested parties to \cite{mystery}.

Results are included for an additional padded variant that uses padding
technique \ref{padding:primitive-array-indirect}. This is intended as a
sanity-check to ensure that each read array
element is in its own cache line. The idea is to avoid values being cached
because they share a cache line with previously read values. As the results
show, this is unnecessary: The technique from \cite{mystery} already achieves
this, as the randomized access pattern causes the previously read cache lines to
be evicted. This is evident by the fact that the padded versions are
approximately twice as slow as the unpadded ones, corresponding to the
additional memory read from the indirection array used for padding.

\begin{code}[h]
\begin{Verbatim}[frame=single]
  private static double jumps(int[] arr) {
    int k = 0;
    for(int j = 0; j < 1 << 25; ++j){
      k = arr[k];
    }
    return k;
  }
\end{Verbatim}
	\caption{Code for measuring memory access times.}
	\label{code:cyclic}
\end{code}

\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{1\textwidth}
		\input{plots/readtimes.tex}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/readtimes2.tex}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/readtimes3.tex}
	\end{subfigure}
	\caption{Random cyclic reads, approximating memory-access times on all 3
	platforms. The y-axes show the average time per read operation, measured
	over $2^{25}$ reads. The x-axes show the working-set size in KiB.}
	\label{fig:readtimes}
\end{figure}

\section{Multicore cache performance}
\label{sec:contrived}

To see the impact of false sharing, it is illustrative to look at a
few contrived example programs. In this section we look at a handful of
variations of programs that perform simple operations in a multicore
setting.

\subsection{Uncontended writes}
The first example we examine illustrates the impact of false sharing by running
simple, uncontended, integer-field increment operations in parallel threads. To see
the impact of false sharing, we observe the time it takes to perform an
increment as a function of the distance between the fields in memory.

\begin{code}
\begin{Verbatim}[frame=single]
  let taskCount parallel tasks do {
    Counter cc = ...;
    for (long i = 0; i < increments_per_thread; i++) {
      cc.value++;
    }
  }
\end{Verbatim}
	\caption{Simplified code for the local-field version of the uncontended-writes
	experiment.}
	\label{code:uncontended}
\end{code}

Each thread performs integer-increment operations. The fields are
uncontended; each thread has its own integer-field and performs no read or write
operations to fields used by the other threads. We run two variants of the
experiment, one where the \java{Counter} instance with the integer field is kept
in a thread-local field, and one where it is read from a shared array in each
iteration. Since each thread operates on its own \java{Counter} instance, there
is no data sharing between CPU cores, and hence no need for synchronization.
Nonetheless, we perform the experiment in variants with and without
\java{volatile} integer declarations, to see the performance impact in both
cases.

Since the fields are uncontended, we will assume the difference in performance
to be a result of unnecessary coherence operations due to false sharing.

In the experiments with \java{volatile} fields, each thread performs 6E6
increments.
In the experiments without \java{volatile}, each thread performs 66E6 increments.
Figure \ref{fig:uncont} and \ref{fig:uncont-nob} show the wall-clock execution
time of the experiments, as a function of padding. Padding technique
\ref{padding:object-array} was used for these experiments. The values chosen for
the amounts of padding are a little odd. This is due to a bug in the padding
code that was discovered late in the project.

The results show significant overhead from false sharing. With volatile
fields, padding reduces execution time by  78-94\%.
With non-volatile fields, padding reduces execution time by 86-94\%, but only
for the array versions. For the local versions with non-volatile fields, padding
reduces execution time by only 12-19\%. The threads never write to the array,
only to the integer fields of the objects pointed to by the array elements, so
no explanation for this behaviour presents itself.

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/barrier.tex}
\caption{Uncontended increments on volatile integer fields. The plot shows wall-clock
	execution time, as a function of the number of bytes used as padding
	between the integers.}
	\label{fig:uncont}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/nobarrier.tex}
\caption{Uncontended increments on non-volatile integer fields. The plot shows
	wall-clock execution time, as a function of the number of bytes used as padding
	between the integers.}
\label{fig:uncont-nob}
\end{figure}

\subsection{Contended writes}

In the previous section we examined the performance of uncontended memory
operations to see the existence and cost of false sharing. In this section, we
shall see that observing the performance of contended memory operations - where
coherence overhead is strictly necessary - can be just as illustrative.

\begin{code}[hbtp]
\begin{Verbatim}[frame=single]
  int sharedInt = 0;
  let taskCount parallel tasks do {
    for (long j = 0; j < increments_per_thread; ++j) {
      sharedInt++;
    }
  }
\end{Verbatim}
	\caption{Simplified code for the local-field version of the contended-writes
	experiment.}
	\label{code:contended}
\end{code}

The code, seen in snippet \ref{code:contended}, is similar to that in snippet \ref{code:uncontended}, but in this
version all threads operate on a single, shared integer field. As in the previous
section, we run two versions of the experiment: One where the field is
\java{volatile}, and one where it is not. As this experiment uses a shared field,
there is no need to store it in an array. However, benchmarks using a
single-element array are included, to show that the behaviour is not
significantly affected by this change.

An additional experiment is included here, running an additional thread that
only reads the integer field. This experiment is the exact same as described in
code snippet \ref{code:contended}, except that when it runs with e.g. four
threads, there is an additional fifth thread running that only performs reads.
This allows us to see that, perhaps contrary to intuition, read operations in a
multicore setting also incur a coherency overhead.

We need the reading thread to run for the full duration of the experiment. To
that end, the thread does not perform a predetermined number of operations, but
is started before the others, and terminated only when the others are
finished. This incurs an overhead, as the thread needs to be stopped before the
benchmark finishes! However, an additional experiment shows that the time it
takes to stop a thread is far too small to significantly skew our results:

The experiments with a volatile field runs 20E6 iterations of the loop;
the experiments with a non-volatile field runs 200E6 iterations of the loop.

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ns) & SD (\%) \\
		\hline
		i5 &  833.4 & 0.33\\
		i7 & 1940.5 & 3.00\\
		Xeon & 1256.7 & 1.08\\
		\hline
		\hline
	\end{tabular}
	\caption{Wall-clock time for for stopping a thread. Time is the total
	wall-clock execution time, the standard deviation (SD) is given in
	percent of the execution time.}
	\label{table:stop-thread}
\end{figure}

\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedintbarrier.tex}
		\caption{i5}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedintbarrierdesktop.tex}
		\caption{i7}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/sharedintbarrierserver.tex}
		\caption{Xeon}
	\end{subfigure}
	\caption{Contended increments on \textbf{volatile} integers. The plots show
	wall-clock execution time in nanoseconds, as a function of the thread count.
	Please note that both axes vary across the plots.}
	\label{fig:cont}
\end{figure}

\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedint.tex}
		\caption{i5}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedintdesktop.tex}
		\caption{i7}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/sharedintserver.tex}
		\caption{Xeon}
	\end{subfigure}
	\caption{Contended increments on \textbf{non-volatile} integers. The
	plots show wall-clock execution time in nanoseconds, as a function of the number of
	threads. Please note that both axes vary across the plots.}
	\label{fig:cont-nob}
\end{figure}

The results, shown in figures \ref{fig:cont} and \ref{fig:cont-nob} show that
memory reads can have a notable performance impact. Both with and without the
\java{volatile} field declaration, we see that adding the additional reading
thread generally increases execution time. In this respect, the plots can only
really be trusted up to about 3, 8 and 48 threads on the i5, i7, and Xeon
platforms, as this about the degree of parallelism we can expect, as shown in
the parallelism experiments previously. With volatile fields, running one
writing and one reading thread takes 3.27-3.97 times as long as running a single
writing thread. Running two writing threads is, of course, even slower, but it
is a significant slowdown from something we might intuitively expect to have no
additional cost. It seems the extra cache invalidations needed to maintain cache
coherence are the culprits. Without the \java{volatile} field declaration,
running one writing and one reading thread takes 14\% longer on the i5 platform,
and 5\% longer on the i7 platform. On the Xeon platform, the additional reading
thread makes the experiment run 0.6\% faster. This is likely just noise. It is
not clear why reading from an array is faster than reading from a field on the
i5 platform.

Just as the extra cost of the writing thread is interesting, it it interesting
to see the cost of additional writing threads. On the i5, i7, and Xeon
platforms, running with two writing threads is 10.48, 8.19, and 8.06 times
slower than running with a single writing thread. This loosely illustrates the
cost of passing cache lines back and forth between two cores: 70.00, 34.39, and
46.50 nanoseconds per increment on average.

\section{Practical applications}
\label{sec:practical}
While incrementing counters can be useful, it is thankfully not the only thing we
build software for.
In this section we examine false sharing in 4 examples of more practical
parallel programs.

First we will look at several programs that build histograms. These example
programs will motivate our approach from coarse- to fine-grained synchronization
with padding. One implementation also serves as an example of false sharing in
lock-free application based on optimistic concurrency with compare-and-swap. We then use Quicksort as an example of the
divide and conquer paradigm, where the division of subproblems limit the need
for synchronization.
As examples of locking applications, we examine an implementation of k-means
clustering, as suggested in the January 2017 exam in Practical Concurrent and
Parallel Programming (PCPP) at the IT University of Copenhagen \cite{kmeansexam,
kmeansexamcode}, as well as two implementations of a concurrent hashmap data structure
used in the same course. Both of these applications are studied in
\cite{mystery}, and can be said to have instigated this project.

\subsection{Histogram builder}

Let us consider the problem of building a histogram: Given a sequence of
integers as input, we wish to build a data structure that maps numbers to their
frequencies in the input sequence.

A simple parallel algorithm for building a histogram presents itself: Divide the
sequence into a number of equal-sized sections, one for each thread. For each
element $a$ in its section, have each thread atomically increment a global
counter (or bucket) representing the frequency of $a$.

Except for the first, all the implementations we consider follow this basic formulation; they differ
only in how they ensure that the increment operations are atomic. We need to
ensure atomicity because the numbers in the input sequence may appear multiple
times, and in different segments, making it possible for more than a single
thread to increment the same frequency counter at the same time.

The histogram benchmarks are performed with the following parameters: An input
sequence which consists of 4 million randomly chosen integers $a\in[0,31]$, and
is divided evenly between threads. The more threads, the less work per thread.
We use 32 buckets: One for each possible number in the input. We will refer to
the number of buckets as the "width" of the histogram.

\subsubsection{Communication-free}
As a baseline for the multicore performance of the histogram problem, we examine
an implementation with minimal communication overhead. The strategy employed can
be thought of as a kind of avoidance or confinement: We arrange it so that part
of the problem can be solved in parallel, \textit{without ongoing communication
between cores}. After the parallel step, a single thread consolidates the
thread-local results produced by each thread. The only necessary communication
or synchronization is making the thread-local results visible to the thread that
executes the consolidation step, \textit{after} the parallel step is completed.
Code snippet \ref{code:histo-lockfree} outlines the implementation of this
approach.

\begin{code}
\begin{Verbatim}[frame=single]
  List<Counter[]> perThreadCounts = ...;
  // Perform thread-local counts in parallel
  let taskCount parallel tasks do {
    // Thread-local counter array. A reference to the
    // same array is stored in perThreadCounts
    Counter[] counters = ...;
    final int from = ..., to = ...;
    for(int j = from; j < to; ++j) {
      counters[inputSequence[j]].value++;
    }
  }
  // Consolidate per-thread counts sequentially
  Counter[] globalCounts = new Counter[WIDTH];
  for(Counter[] localCounts : perThreadCounts) {
    for(int i = 0; i < WIDTH; ++i){
      globalCounts[i].value += localCounts[i].value;
    }
  }
\end{Verbatim}
	\caption{Simplified code for the communication-free version of the
	histogram builder.}
	\label{code:histo-lockfree}
\end{code}

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ms) & SD (\%) \\
		\hline
		\input{plots/histo-local-lockfree}
		\hline
		\hline
	\end{tabular}
	\caption{Execution times for the communication-free histogram builder. Time
is the total wall-clock execution time, the standard deviation (SD) is given in
percent of the execution time.} \label{table:histo-lockfree}
\end{figure}

As we will see, this is by far the fastest of our solutions to the histogram. It
also scales relatively well with the number of cores.

\subsubsection{Coarse-grained locking}
When it comes to locking implementations, the simplest solution is for the
threads to take a single, shared lock anytime they increment a frequency
counter. This solution is slow. At any time, only one thread can be incrementing a counter.

\begin{code}
\begin{Verbatim}[frame=single]
  Object lock = new Object();
  let taskCount parallel tasks do {
    final int from = ..., to = ...;
    for(int j = from; j < to; ++j) {
      synchronized(lock){
        counters[inputSequence[j]].value++;
      }
    }
  }
\end{Verbatim}
	\caption{Simplified code for the threads in the coarse-grained locking
	version of the histogram builder.}
\end{code}

The table in figure \ref{table:histo-global} shows the execution times of this
solution to the histogram problem. At we would expect, it scales poorly with the number of cores.

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ms) & SD (\%) \\
		\hline
		\input{plots/histo-lock-global}
		\hline
		\hline
	\end{tabular}
	\caption{Execution times for the histogram builder using a single
global lock. Time is per input-element per thread, the standard deviation (SD)
is given in percentage of the execution time.}
	\label{table:histo-global}
\end{figure}

We will not measure it, but there is a possibility for false sharing in this
implementation. Consider the following sequence of operations, where we assume
counter0 and counter1 are in the same cache line:

\begin{enumerate}
	\item CPU0 updates counter 0. The cache line is left in CPU0's cache.
	\item \label{list:invalidation} CPU1 updates counter 1. The cache line is invalidated in CPU0's
		cache.
	\item \label{list:unneededread} CPU0 updates counter 0. The counter is
		invalid in CPU0's cache, and
		must be read from CPU1's cache instead.
\end{enumerate}

The cache miss in step \ref{list:unneededread} is unnecessary. It only occurs
because the two counters share a cache line, and because the other counter was
updated in step \ref{list:invalidation} by a different CPU core. The impact of
false sharing here is likely overshadowed by the poor choice of locking scheme.

There is another kind of unnecessary/false communication between CPU cores here,
that has nothing to do with sharing of cache lines: When a thread takes the
lock, updates from previous threads are guaranteed to be made visible to it.
Except for updates to the counter the thread is \textit{trying} to increment,
this is unnecessary.

\subsubsection{Fine-grained locking}
\label{sec:fine-grained-histo}
A better solution is to have different counters guarded by different locks. This
way, threads can perform their work in parallel, except when different
threads work on counters guarded by the same lock. For practical applications
the ideal degree of granularity should be determined by experiment. In this
experiment I use 32 locks: One lock for each bucket.

\begin{code}
\begin{Verbatim}[frame=single]
  Object[] locks = ...;
  let taskCount parallel tasks do {
    final int from = ..., to = ...;
    for(int j = from; j < to; ++j) {
      int a = inputSequence[j];
      synchronized(locks[a]){
        counters[a].value++;
      }
    }
  }
\end{Verbatim}
	\caption{Simplified code for the threads in the fine-grained locking
	version of the histogram builder.}
\end{code}

The improved level of parallelism increases the possibility for false sharing: In
the previous version only one thread could perform writes at a time. In this
version, threads holding different locks can in theory spend arbitrary amounts
of time invalidating each other's cache lines without doing actually
incrementing the counterss.

There are two clear candidates for false sharing: The locks and the counters.
Neither are stored directly in the arrays: Locks must be object types, and are
hence stored as references, and the integers used for the counters are stored in
placeholder objects so we can declare them as \java{volatile}. False sharing of
the references stored in the arrays should therefore be irrelevant, but if the
objects are densely allocated, e.g. in a tight for-loop, they can still be
placed back-to-back in memory.

We measure the impact of false sharing by benchmarking versions with different
amounts of padding between the locks and counters. We use padding technique
\ref{padding:object-array-indirect} for this experiment.

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r r}
		\hline
		\hline
		Platform & Time wo. padding (ms) & Best time (ms) & Improvement \\
		\hline
		i5 & 113.0 & 100.88 & 10.7\% \\
		i7 & 71.88 & 59.9 & 16.9\% \\
		Xeon & 420.8 & 118.62 & 71.8\% \\
		\hline
		\hline
	\end{tabular}
	\caption{Best times vs. times without padding for the fine-grained
	histogram builder. Times are wall-clock execution times. Improvement is
	given in percent of the unpadded execution time: An improvement of
	71.8\% indicates that at least 71.8\% of the execution time is spend on
	unnecessary coherence communication in the unpadded version.}
	\label{table:hist-local}
\end{figure}

Figures \ref{fig:histo-local-i5}, \ref{fig:histo-local-i7}, and
\ref{fig:histo-local-xeon} show the wall-clock execution times using different
amounts of padding for the i5, i7, and Xeon platforms respectively. Each of the smaller
plot shows the execution time of the histogram builder, as a function of the
amount of padding between the counters. The third axis -- the amount of padding
between the locks -- is unrolled into the different plots.

The table in figure \ref{table:hist-local} compares the unpadded times with the
best times for each platform.

The effect is smallest on the i5 platform: Using 128 bytes padding between
counters, and no padding between locks, yields a 10.7\% improvement on not using
padding. Padding the locks does not seem to benefit performance, and even makes
it worse in some cases.

On the i7 platform, padding locks and counters both benefits performance.
Padding 128 bytes between counters and 112 bytes between locks yields a 16.9\%
percent improvement on not using padding.

The most significant effect is seen on the Xeon platform. Like on the i7
platform, using padding is beneficial for both data structures, but here the
benefit from padding the locks is much more pronounced. Using 128 bytes of
padding for the counters and 112 bytes for the locks yields a 71.8\% improvement
on not using padding, indicating that most of the time is spent on cache
coherence overhead in the unpadded version.

There is a noteworthy caveat: The experiment runs with 48 threads on the Xeon
platform, but there are only 32 locks.
This cannot not be solved simply by increasing the
number of locks, as there are still only 32 distinct values in the input, and
therefore only 32 counters.
The scarcity of locks likely limits
throughput, but the false sharing impact should be the same without
this limitation, which is what we are concerned with.

It is not surprising that the platform with the most cores suffers the worst.
The more threads running in parallel, the bigger the likelihood of invalidating
a cache line that is relevant to another core. Furthermore, the invalidation
messages must be sent to all the other cores. Processors must wait for
acknowledgement messages from all the other cores, so there is a lot more
communication occurring due the coherence protocol. The larger cache hierarchy
likely also means larger physical distances, which in turns makes communication
slower.

While our observations seem to agree with our understanding of false sharing,
two new mysteries present themselves: It is not clear why the execution time
keeps improving past 64-bytes padding, and it is not clear why padding the locks
has such a modest impact.

We would expect the performance to stop improving abruptly around 64
bytes, as that is the cache line size used by our hardware architectures.
One possible explanation for the continued improvements is the cache prefetch mechanism.
It is possible that the prefetch mechanism causes more than a single cache line
to be fetched. This is the explanation given for why the \java{@Contended}
annotation causes 128 bytes of padding to inserted on OpenJDK, under the
assumption that cache lines are half that size\cite{openjdkmailcontended}.
This explanation is in agreement with the Intel optimization
manual\cite{inteloptimize}, which states that the spatial prefetcher
"strives to complete every cache line fetched to the L2 cache with the pair line
that completes it to a 128-byte aligned chunk"\footnotemark. While this
prefetching does not incur a false sharing overhead, it will leave an additional
cache line in the CPU's cache. Another CPU might then later write to that
cache line, incurring unnecessary coherence overhead. The additional padding
does not prevent the prefetcher from loading the irrelevant cache line, but it
does prevent other threads from ever writing to it. We can think of this problem
as false sharing of the chunks of the L2 cache's spatial prefetcher.

An explanation for the other mysterious artifact, the modest impact of padding
the locks, is more elusive. It seems to make sense that using a small number of
threads compared to the number of locks limits the impact of false sharing, as
it lessens the likelihood of threads working on the same cache lines. This
intuition fails to explain why the impact of padding the counters is larger: The
memory layout of the locks should be exactly the same as that of the counters,
and there are equally many of them.

\footnotetext{It is not entirely clear from the manual which microarchitectures
this applies to. It is stated for the Sandy Bridge microarchitecture, and seems
to apply to those that succeed it as well.}

\input{histo-plots.tex}

\subsubsection{Compare-and-swap}
Following the same strategy as the fine-grained locking implementation
, we can use optimistic concurrency to implement a lock-free version of
the histogram.

Optimistic compare-and-swap works best when the operation we perform is cheap,
and the resource we perform it on has low contention. That way, retries will
happen rarely, and be cheap when they do.

The histogram problem lends itself well to a compare-and-swap based solution:
All writes to shared data are the results of simple increment operations, which
are fast. Resource contention depends on the ratio of counters to threads, and the
order of the input.

\begin{code}
\begin{Verbatim}[frame=single]
  private AtomicIntegerArray counters = ...;
  let taskCount parallel tasks do {
    final int from = ..., to = ...;
    for(int j = from; j < to; ++j) {
        counters.getAndIncrement(inputSequence[j]);
    }
  }
\end{Verbatim}
	\caption{Simplified code for the threads in the compare-and-swap 
	version of the histogram builder.}
	\label{code:histo-cas}
\end{code}

Code snippet \ref{code:histo-cas} outlines the compare-and-swap implementation
of the histogram builder. It uses Java's \java{AtomicIntegerArray} class to
hold counters. The \java{getAndIncrement} method performs the atomic compare-and-swap
operation, and guarantees that updates are visible.

Reading the source code for the OpenJDK implementation of the
\java{AtomicIntegerArray} class \cite{atomicinterarraysrc} reveals that
the integers are stored in a plain \java{int} array.
To ensure atomicity and visibility of updates, the implementation relies on the
undocumented \java{sun.misc.Unsafe} class. This strongly suggests that our
implementation suffers from false sharing of cache lines, as the array elements
appear to be densely allocated. To confirm this suspicion, we run experiments with
padding technique \ref{padding:primitive-array-indirect}.

Figures \ref{fig:histo-cas-i5}, \ref{fig:histo-cas-i7}, and
\ref{fig:histo-cas-xeon} show that the execution time of this version of the
histogram builder improves drastically when we introduce padding between the
counters, confirming our suspicions that the \java{AtomicIntegerArray} class,
and therefore our histogram builder, suffers from false sharing.

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Best time (ms) & Padding (bytes)\\
		\hline
		i5 & 60.62 & 48 \\
		i7 & 25.26 & 128 \\
		Xeon & 48.62 & 128 \\
		\hline
		\hline
	\end{tabular}
	\caption{The best execution times for the compare-and-swap histogram
	builder, and the amounts of padding used to achieve them. Times
	are wall-clock execution times.}
	\label{table:histo-cas}
\end{figure}

The best execution times, shown in figure \ref{table:histo-cas}, are 
better than those of the fine-grained locking implementation by factors of 1.9,
2.8, and 8.7 for the i5, i7, and Xeon platforms respectively. However, the
communication-free implementation is still 3.3, 3.3, and 9.7 times faster than
the compare-and-swap version.

The lesson here is that data-sharing between cores should be avoided
entirely when doing so is easy. And when it cannot easily be avoided, we
should at least avoid unnecessary communication caused by false sharing.

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/histo-cas-i5.tex}
\caption{Execution times for the compare-and-swap based histogram builder on the i5 platform. The plot shows the
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:histo-cas-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/histo-cas-i7.tex}
\caption{Execution times for the compare-and-swap based histogram builder on the i7 platform. The plot shows the
	wall-clock execution time in ns., as a function of padding in bytes.}
	\label{fig:histo-cas-i7}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/histo-cas-xeon.tex}
\caption{Execution times for the compare-and-swap based histogram builder on the Xeon platform. The plot shows the
	wall-clock execution time in ns., as a function of padding in bytes.}
	\label{fig:histo-cas-xeon}
\end{figure}

\subsection{Quicksort}
A famous example of a divide-and-conquer algorithm, Quicksort recursively
divides the sorting problem into independent subproblems, combining partial
results into a solution for the whole problem. We can see this problem division
as the same kind of avoidance we saw in the communication-free histogram
example: If the sub-problems are independent, they can be solved in parallel on
a multicore system without any communication between parallel processes. Of
course, we need to ensure that solutions are visible \textit{after} they are
found, but no communication is needed while the threads work. This is the only
of our experiments where eliminating false sharing does not significantly
improve execution times.

\begin{code}
\begin{Verbatim}[frame=single]
  public class QuickSort extends RecursiveAction {
    ...
    protected void compute() {
      if (hi-lo <= cutoff) {
        SelectionSort.sort(array,lo,hi);
        return;
      }
      int mid = partition();
      ForkJoinTask<Void> leftTask =
        new QuickSort(array, lo, mid, cutoff).fork();
      QuickSort right =
        new QuickSort(array, mid + 1, hi, cutoff);
      right.compute();
      leftTask.join();
    }
    ...
  }
\end{Verbatim}
	\caption{Simplified code for the Quicksort problem. The left-out
	\java{SelectionSort.sort} method implements sequential Selection sort.
	The left-out \java{partition} method implements a median-of-three
	version of Hoare partitioning.}
	\label{code:qsort}
\end{code}

The experiments use a (somewhat naive) parallel implementation of quicksort,
outlined in code snippet \ref{code:qsort}.
Anytime the algorithm divides the problem in two, the calling thread forks a
task to solve the left part of the problem, executing the right part itself.
In this experiment we do not directly control the number of threads used on each
platform: Forked tasks are run in parallel at the discretion of the
\java{ForkJoinPool}.

As is common, and as is recommended in the popular algorithms text book by
\citet{sedgewick}, the implementation uses a cutoff to a sequential Selection
sort. An experiment is included to find a good value for the cutoff.

There is no step to combine the results of two subproblems, as calls operate
in-place on the same shared array. While this may \textit{seem} like communication
between threads, there is only a single thread operating on a given
array-segment at any time. The only relevant communication is that of the
constructor parameters to the recursive calls, and the reference to the array
itself. Operations in recursive calls are guaranteed to be visible to the caller
because of the call to \java{join()}.

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/qsort.tex}
	\caption{Quicksort on the i5 platform. The plot shows wall-clock execution time in
	ns., as a function of padding in bytes. For the padnone experiment, the
	x-axis indicates the cutoff value in bytes.}
	\label{fig:qsort-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/qsort-desktop.tex}
	\caption{Quicksort on the i7 platform. The plot shows wall-clock execution time in
	ns., as a function of padding in bytes. For the padnone experiment, the
	x-axis indicates the cutoff value in bytes.}
	\label{fig:qsort-i7}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/qsort-server.tex}
	\caption{Quicksort on the Xeon platform. The plot shows wall-clock execution time in
	ns., as a function of padding in bytes. For the padnone experiment, the
	x-axis indicates the cutoff value in bytes.}
	\label{fig:qsort-xeon}
\end{figure}

There is a risk of false-sharing every time the algorithm subdivides the input
array: The rightmost elements of the left part may share a cache line with the
leftmost elements of the right part. We run three experiments on each platform:
\begin{description}
	\item[padnone] With no padding, used to determine a good value for the cutoff.
	\item[padall] Where padding is added between all elements in the input
		array.
	\item[padsome] Where padding is added between some elements in the input
		array, such that padding segments and data segments are of equal
		length.
\end{description}
We use padding technique
\ref{padding:primitive-array-indirect} to space the array elements.

Experiments are run using an array with 4 million integers to be sorted. Padall
and padsome experiments use a cutoff value of 16 array elements, corresponding
to 64 bytes. Figures \ref{fig:qsort-i5}, \ref{fig:qsort-i7},
\ref{fig:qsort-xeon} show the execution times of the experiments. The results
show that adding padding between all array elements vastly increases execution
time, while adding padding between just some elements decrease execution time by
0-1.8\%

There are a few reasons why false sharing might not introduce significant
communication overhead to the Quicksort implementation: At most one cache line 
can be falsely shared at each problem division. This means each task
accesses at most 2 cache-lines that are falsely shared with other threads. An
additional two cache lines per task may be subject
to false invalidations due to the L2 prefetcher, as explained in
section \ref{sec:fine-grained-histo}. Each task writes to each of their array
elements at most once in the partitioning step. When performing
Selection sort, each task performs up to $\frac{16\cdot17}{2} = 136$ writes, but all
within the same 64-byte memory segment, so at most 2 cache lines are involved.
As a result, the shared cache lines suffer contention, if any. Finally, nothing
guarantees visibility of updates before a task has finished, so we do not suffer
the full overhead of the cache coherence protocol.

The fact that we see slight improvement from padding in the padsome experiments
might simply be noise, but it leads us to a new question. The experiments are
all but guaranteed to have useless padding. That is, padding between elements
that are always in the same subdivisions. With Quicksort, we cannot predict
where the array will be divided beforehand, but perhaps other divide-and-conquer
algorithms, such as Mergesort, could benefit more from eliminating false
sharing, simply because we could add padding only in positions where the input
array gets split.

The plots show a high standard variations for some amounts of padding. On the
Xeon platform in particular, the values for the padall experiment are highly
unreliable. This may indicate that there is something wrong with my benchmarks,
or that there is otherwise something more than meets the eye here.

\subsection{K-means}
The K-means problem from the 2017 PCPP exam -- the original inspiration for
\textit{A multicore performance mystery solved}\cite{mystery} and this report --
is another example of a locking application which suffers from false sharing.

The k-means clustering algorithm takes as input: A list of points to be
clustered, and a list of $k$ initial cluster means. The algorithm then
assigns each point to its nearest cluster, and updates the clusters'
means according to the new assignments. This process is repeated iteratively
until cluster means have stabilised.

All our k-means experiments are run with 200.000 points and 81 clusters, and
take 108 iterations to complete.

\begin{code}
\begin{Verbatim}[frame=single]
  public static class Cluster implements Cluster{
    private volatile Point mean;
    private double sumx, sumy;
    private int count;
    public synchronized void addToMean(Point p) {
      sumx += p.x;
      sumy += p.y;
      count++;
    }
    public synchronized boolean computeNewMean() {
      ...
    }
\end{Verbatim}
	\caption{Simplified code for the k-means \java{Cluster} class}
	\label{code:cluster}
\end{code}

\begin{code}
\begin{Verbatim}[frame=single]
  while (!converged) {
    // Assignment step: put each point in exactly one
    // cluster
    let taskCount parallel tasks do {
      final int from = ..., to = ...;
      for (int pi=from; pi<to; pi++) 
        myCluster[pi] = closest(points[pi], clusters);
    }
    // Update step: recompute mean of each cluster
    let taskCount parallel tasks do {
      for (int pi=from; pi<to; pi++)
        myCluster[pi].addToMean(points[pi]);
    }
    converged = true;
    for (NormalCluster c : clusters)
      converged &= c.computeNewMean();
  }
\end{Verbatim}
	\caption{Simplified code for the original k-means implementation, KMeans2P.}
	\label{code:kmeans2p}
\end{code}

\begin{code}
\begin{Verbatim}[frame=single]
  while (!converged) {
    // Assignment step: put each point in exactly one
    // cluster
    let taskCount parallel tasks do {
      final int from = ..., to = ...;
      for (int pi=from; pi<to; pi++)
        closest(points[pi], clusters)
          .addToMean(points[pi]);
    }
    // Update step: recompute mean of each cluster
    converged = true;
    for (Cluster c : clusters)
      converged &= c.computeNewMean();
  }

\end{Verbatim}
	\caption{Simplified code for the optimized k-means implementation, KMeans2Q.}
	\label{code:kmeans2q}
\end{code}

The code used for this experiment is the same as in \cite{mystery}, with small
adaptations, e.g. to the fields used for padding. The padding technique used is
technique \ref{padding:object-fields}. In fact, the example given for the
technique in chapter \ref{chap:javamem} is taken from KMeans2Q64.

We examine 4 K-means implementations:

\begin{description}
	\item[KMeans2P] {The unoptimized implementation described
		in\cite{mystery} and outlined in code snippet
		\ref{code:kmeans2p}.}
	\item[KMeans2Q] {The optimized implementation described in \cite{mystery}
		and outlined in  code snippet \ref{code:kmeans2q}.}
	\item[KMeans2Q64] {The same as KMeans2Q, but with a \java{Cluster} class
		with two 64-byte padding segments.}
	\item[KMeans2Q128] {The same as KMeans2Q, but with a \java{Cluster} class
		with two 128-byte padding segments.}
\end{description}

The most significant false sharing overhead comes from the \java{sumx},
\java{sumy}, \java{count}, and \java{mean} fields in the \java{Cluster}
class (Hereinafter, we shall refer to the former three of these fields as
the \textit{assignment fields}). To see the problem, we need to understand just
3 points:

\begin{enumerate}
	\item{Updates to the assignment and \java{mean} fields
		are subject to the cache coherence protocol:
		They are guarded by a lock (the surrounding \java{Cluster} instance),
		and the \java{mean} field is declared \java{volatile}}.
		Writes to these fields will result in invalidation
		messages being sent to the other cores. Reads from \java{mean}
		will force the CPU core to process pending invalidations, which
		may result in later cache misses.
	\item {The \java{mean} fields might be in the same cache line as the
		assignment fields. The fields consist of an object pointer, two
		\java{doubles} and an \java{int}, taking up a total of 28
		bytes. The small memory footprint even makes it possible for
		fields in two separate \java{Cluster} to share a cache
		line!}
	\item {Every call to the \java{closest} method reads the \java{mean}
		fields of \textit{every cluster}. Calls to the \java{closest}
		and \java{addToMean} methods are interleaved in KMeans2Q, which
		means writes to the assignment fields happen concurrently with
		a lot of reads from the \java{volatile mean} field.}
\end{enumerate}

This means that every write to the assignment fields runs the risk of causing
false cache misses on all other CPU cores, if the assignment fields falsely
share the cache lines of \java{mean} fields.

The padded versions of KMeans2Q arrange the fields so that the assignment fields
are together in a contiguous memory segment, with padding both before and after
that segment. That should guarantee that the \java{mean} field is never in the
same cache line as the three other fields. No padding is added between the
\java{sumx}, \java{sumy}, and \java{count} fields. These three fields are always
updated together, so we welcome the possibility of them being placed in the
same cache line.

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		K-means version & Time (ms) & SD (\%) \\
		\hline
		\input{plots/peterskmeans-i5}
		\hline
		\hline
	\end{tabular}
	\caption{The k-means problem on the i5 platform, using 4 tasks. Time is
	the total wall-clock execution time, the standard deviation (SD) is
	given as the percentage of the execution time.}
	\label{table:kmeans-i5}
\end{figure}

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		K-means version & Time (ms) & SD (\%) \\
		\hline
		\input{plots/peterskmeans-i7}
		\hline
		\hline
	\end{tabular}
	\caption{The k-means problem on the i7 platform, using 8 tasks.  Time is
	the total wall-clock execution time, the standard deviation (SD) is
	given as the percentage of the execution time.}
	\label{table:kmeans-i7}
\end{figure}

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		K-means version & Time (ms) & SD (\%) \\
		\hline
		\input{plots/peterskmeans-xeon}
		\hline
		\hline
	\end{tabular}
	\caption{The k-means problem on the Xeon platform, using 48 tasks.
	Time is the total wall-clock execution time, the standard deviation (SD)
	is given as the percentage of the execution time.}
	\label{table:kmeans-xeon}
\end{figure}

The results, shown in figures \ref{table:kmeans-i5}, \ref{table:kmeans-i7}, and
\ref{table:kmeans-xeon}, show that false sharing has a significant effect on our 
k-means implementations. Particularly on the Xeon platform, where the KMeans2Q128
is 2.1 times faster than KMeans2Q. Curiously, the execution time doesn't get as
close to that of KMeans2P as in \citeauthor{mystery}'s experiments, even though
both experiments use two 128-byte padding segments in the \java{Cluster} class.

\subsection{Striped hashmaps}
Another problem used in the PCPP course, concurrent hashmap data structures
provide a more complex example of striped locking applications than the
histogram builder we examined earlier: Hashmaps serve as general key-value
stores, their sizes may change dynamically, and unlike histograms, reads
and writes to hashmaps are often interleaved.

We consider two versions of the striped hashmap: Striped map, and striped-write
map. Both versions are from the PCPP course, and both versions are examined in
\cite{mystery}. However, \cite{mystery} examines only the striped-write map
with respect to false sharing.

Both implementations follow the same overall strategy: Key-value pairs are
stored in buckets, chosen by hashing they key. Buckets are implemented as linked
lists, allowing a single bucket to hold multiple key-value pairs. This
takes care of hash collisions. Each bucket is assigned to a stripe, and each
stripe is associated with a single lock that guards operations on buckets in
that stripe.

The code included here is minimal. The hashmap implementations are still used as
exercises in the PCPP course, and I do not wish to deprive students of the
satisfaction of completing them. The full implementations are fairly intricate,
but to see the risk of false sharing overhead, we need only consider the data
structures they use internally.

Inspired by the \java{java.util.concurrent.atomic.LoncAdder}
class\footnotemark{}, we use a quasi thread-local counter as the stress pattern for our hashmap
experiments. Each thread reads its own segment of the input, and stores the sum
of the inputs in a shared hashmap. Each thread uses a unique thread-id as key
for its own counter. Whether the counters are effectively thread-local depends
on whether the thread-ids hash to buckets in the same stripe.
Experiments are run with 32 stripes on all platforms, regardless of thread
count. The input sequence consists of 33 million integers, divided evenly
between threads.

\footnotetext{While the LongAdder class ostensibly works in the same way as this
experiment, it does not use a hashmap to store the counters, and uses an
intricate optimistic concurrency scheme for updates.}

\begin{code}
\begin{Verbatim}[frame=single]
  let taskCount parallel tasks do {
      final int threadId = ...;
      final int to = ..., from = ... ;
      map.put(threadId, 0);
      for(int j = from; j < to; ++j) {
        int a = inputSequence[j];
        map.put(threadId, map.get(threadId) + num);
      }
      threads.add(t);
    }
\end{Verbatim}
	\caption{Simplified code for the quasi thread-local counter we use for
	the hashmap experiments.}
\end{code}

\begin{code}
\begin{Verbatim}[frame=single]
  public static class StripedMap<K,V> {
    private volatile ItemNode<K,V>[] buckets;
    private Object[] locks;
    private final int[] sizes;
    ...
  }
\end{Verbatim}
	\caption{The most significant fields in the StripedMap class.}
\end{code}

\subsubsection{Striped map}
The striped map relies on locking for both reading and writing operations (such
as \java{get} and \java{put}). This ensures mutual exclusion as well as
visibility of updates.

The elements of the \java{locks} array pose a risk of false sharing: An
\java{Object} instance takes up 16 bytes\footnotemark, which means we can fit
up to four locks in a single 64-byte cache line. Taking or releasing a lock can
then falsely invalidate three other locks in the caches of other CPU cores.

The \java{buckets} and \java{sizes} fields are also candidates for false
sharing, but are unlikely to cause significant overhead.

\footnotetext{In chapter \ref{chap:javamem}, we saw that this is the case on our
three platforms, but it depends on the specific Java runtime platform.}

Figures \ref{fig:hashmap-striped-i5} and \ref{fig:hashmap-striped-i7-xeon} show
the wall-clock execution time when using the striped hashmap with different
amounts of padding between the locks. The locks are padded with padding
technique \ref{padding:object-array-indirect}.

\begin{code}
\begin{Verbatim}[frame=single]
  public static class StripedWriteMap<K,V> {
    private volatile ItemNode<K,V>[] buckets;
    private Object[] locks;
    private AtomicIntegerArray sizes;
    ...
  }
\end{Verbatim}
	\caption{The most significant fields in the StripedWriteMap class.}
\end{code}

\subsubsection{Striped-write map}
The striped-write map works in much the same way as the striped map, except it
lowers lock contention by not taking locks for read operations like \java{get}.
Visibility of writes is guaranteed by piggy-backing on the visibility
guarantees of the \java{AtomicIntegerArray} class, used to store element-counts
for each bucket. One disadvantage of this technique is that the \java{sizes}
elements now effectively become \java{volatile}, causing a larger coherence
overhead.

Figure \ref{fig:hashmap-stripedwrite} shows the wall-clock execution time when
using the striped-write hashmap with different amounts of padding between the
locks and elements of \java{sizes}. The locks are padded using padding technique
\ref{padding:object-array-indirect}. The elements of sizes are padded by reusing the
indirection array from the locks. This is equivalent to using padding technique
\ref{padding:primitive-array-indirect}, but without the overhead of using the
additional indirection array.

The experiments show that false sharing has a pronounced impact on both hashmaps
across all three platforms, as simply padding elements of shared datastructures
improve execution times.
The biggest improvement is with the striped-write map on the Xeon platform.
Here, using 112 bytes of padding reduces the execution time by 33\%. The
smallest improvement is with the striped hashmap on the i5 platform. Here, using
48 bytes of padding reduces the execution time by 17\%.

There is an interesting artifact in the results: The striped-write map performs
remarkably worse than the striped map in these experiments. This may be
explained by its use of an immutable implementation of the \java{ItemNode}
class: Each write allocates new nodes, incurring both the cost of the
allocations and of garbage collecting the old nodes. A stress pattern with a
higher ratio of read- to write write-operations (the ratio in our thread-local
counter is $\sim 1/1$) should benefit from using the striped-write map, as the
cost saved by not locking eclipses the cost of the extra allocations.

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/hashmap-striped-i5.tex}
\caption{The striped hashmap problem on the i5 platform. The plot shows
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:hashmap-striped-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/hashmap-striped-i7-xeon.tex}
\caption{The striped hashmap problem on the i7 and Xeon platforms. The plot shows
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:hashmap-striped-i7-xeon}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/hashmap-stripedwrite.tex}
\caption{The striped-write hashmap problem on all 3 platforms. The plot shows
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:hashmap-stripedwrite}
\end{figure}

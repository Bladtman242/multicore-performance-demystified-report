\chapter{Experiments}
\label{sec:experiments}

\section{Preliminary Experiments}

\subsection{CPU Idle Load}
\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{1\textwidth}
		\input{plots/cpuloadx1.tex}
		\caption{i5}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/cpuload-desktop.tex}
		\caption{i7}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/cpuload-server.tex}
		\caption{Xeon}
	\end{subfigure}
	\caption{CPU baselines for benchmarks. The x-axes show CPU load in
	percent, as reported by the operating system (100\% load corresponds to
	all cores being busy 100\% of the time). The y-axes show time in minutes.}
	\label{fig:cpuload}
\end{figure}

\subsection{Degrees of Parallelism}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/paral.tex}
	\caption{Parallelism experiment on the i5 platform. The plot shows
	wall-clock execution time in ns., as a function of thread count.}
	\label{fig:parallel-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/paral-desktop.tex}
	\caption{Parallelism experiment on the i7 platform. The plot shows
	wall-clock execution time in ns., as a function of thread count.}
	\label{fig:parallel-i7}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/paral-server.tex}
	\caption{Parallelism experiment on the Xeon platform. The plot shows
	wall-clock execution time in ns., as a function of thread count.}
	\label{fig:parallel-xeon}
\end{figure}

\subsection{Memory Hierarchy Access-Times}

\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{1\textwidth}
		\input{plots/readtimes.tex}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/readtimes2.tex}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/readtimes3.tex}
	\end{subfigure}
	\caption{Random cyclic reads, approximating memory-access times on all 3
	platforms. The y-axes show the average time per read operation, measured
	over $2^{25}$ reads. The x-axes show the working-set size in KiB.}
	\label{fig:readtimes}
\end{figure}

\section{Multicore Cache Performance}

To see the possible impact of false sharing, it is illustrative to look at a
few contrived example programs. In this section we look at a handful of
variations of programs that perform simple operations in a multicore
setting.

\subsection{Uncontended Writes}
The first example we examine illustrates the impact of false sharing by running
simple, uncontended, integer-field increment operations in parallel threads. To see
the impact of false sharing, we observe the time it takes to perform an
increment as a function of the distance between the fields in memory.

\mynote{add code excerpt around here}

Each thread performs millions of increments. The fields are
uncontended; each thread has its own integer-field and performs no read or write
operations to fields used by the other threads. Since each thread operates on
its own field, there is no need for synchronization. Nonetheless, we perform
the experiment in variants with and without \java{volatile} integer declarations, to
see the performance impact in both cases.

Since the fiends are uncontended, we will assume the difference in performance
to be a result of unnecessary coherence operations due to false sharing.

In the experiments with volatile integers, each thread performs 6M increments.
In the experiments without volatile, each thread performs 66M increments. The
experiments are run with 4, 8, and 48 threads on the i5, i7, and Xeon platforms,
respectively.

Figure \ref{fig:uncont} and \ref{fig:uncont-nob} show the average time per
increment, as a function of padding.

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/barrier.tex}
\caption{Uncontended increments on volatile integers. The plot shows wall-clock
	execution time, as a function of the number of bytes used as padding
	between the integers.}
	\label{fig:uncont}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/nobarrier.tex}
\caption{Uncontended increments on non-volatile integers. The plot shows
	wall-clock execution time, as a function of the number of bytes used as padding
	between the integers.}
\label{fig:uncont-nob}
\end{figure}

\mynote{Add explanation of the different plots (array/local,
barrier/no-barrier), include values of plots (plots are hard to read), and
reflect/conclude}

\subsection{Contended Writes}

In the previous section we examined the performance of uncontended memory
operations to see the existence and cost of false sharing. In this section, we
shall see that observing the performance of contended memory operations - where
coherence overhead is strictly necessary - can be just as illustrative.

\mynote{include code snippet}

The program is similar to \mynote{reference uncontended snippet}, but in this
version all threads operate on a single, shared integer field. As in the previous
section, we run two versions of the experiment: One where the field is
\java{volatile}, and one where it is not. As this experiment uses a shared field,
there is no need to store it in an array. However, benchmarks using a
single-element array are included, to show that the behaviour is not
significantly affected by this change.

An additional experiment is included here, running an additional thread that
only reads the integer field. This allows us to see that, perhaps contrary to
intuition, read operations in a multicore setting also incur a coherency
overhead.

\mynote{include getter/stop code snippet}

We need the reading thread to run for the full duration of the experiment. To
that end, the thread does not perform a predetermined number of operations, but
is started before the others, and terminated only when the others are
finished. This incurs an overhead, as the thread needs to be stopped before the
benchmark finishes! However, an additional experiment has shown that the time it
takes to stop a thread is far too small to significantly skew our results.

\mynote{include table of numbers (they are 833.4$\pm$2.82 (desktop),
1940.5$\pm$58.12 (server),
and 1256.7$\pm$13.59 (laptop) ns)})

\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedintbarrier.tex}
		\caption{i5}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedintbarrierdesktop.tex}
		\caption{i7}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/sharedintbarrierserver.tex}
		\caption{Xeon}
	\end{subfigure}
	\caption{Contended increments on \textbf{volatile} integers. The plots show
	nanoseconds per increment, as a function of the thread count.
	Please note that both axes vary across the plots.}
	\label{fig:cont}
\end{figure}

\begin{figure}[hbpt]
	\graphicspath{{plots/}}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedint.tex}
		\caption{i5}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\input{plots/sharedintdesktop.tex}
		\caption{i7}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\input{plots/sharedintserver.tex}
		\caption{Xeon}
	\end{subfigure}
	\caption{Contended increments on \textbf{non-volatile} integers. The
	plots show nanoseconds per increment, as a function of the number of
	threads. Please note that both axes vary across the plots.}
	\label{fig:cont-nob}
\end{figure}

\mynote{include plots as ns-per-operation, concrete numbers/factors, and
discussion/conclusion on the results, concrete parameters: work not divided..}

\mynote{Konklusioner: Læsning er ikke gratis(!), pris for contended writes (både
med og uden barrier/volatile) siger en del om MESI pris (1 vs 2 tråde er
illustrativt), indikerer (løst) pris af at smide en cache-line frem og tilbage
mellem kerner}


\section{Practical Applications}
While incrementing counters is useful, it is thankfully not the only thing we
build software for.
In this section we examine false sharing in 4 \mynote{Update if it doesn't
end up 4} examples of more complicated parallel programs.

First we will look at several programs that build histograms. These example
programs will motivate our approach from coarse- to fine-grained synchronization
with padding. One implementation also serves as an example of false sharing in
lock-free CAS based applications. We then use Quicksort as an example of the
divide and conquer paradigm, where the division of subproblems limit the need
for synchronization.
As examples of locking applications, we examine an implementation of k-means
clustering, as suggested in the January 2017 exam in Practical Concurrent and
Parallel Programming (PCPP) at the IT University of Copenhagen \cite{kmeansexam,
kmeansexamcode}, as well as implementation of a concurrent hashmap datastructure
used in the same coarse. Both of these applications are studied in
\cite{mystery}, and can be said to have instigated this project.

\subsection{Histogram builder}

Let us consider the problem of building a histogram: Given a sequence of
integers as input, we wish to build a data structure that maps numbers to their
frequencies in the input sequence:

A simple parallel algorithm for building a histogram presents itself: Divide the
sequence into a number of equal-sized sections, one for each thread. For each
element $a$ in its section, have each thread atomically increment a global
counter (or bucket) representing the frequency of $a$.

All the implementations we consider follow this basic formulation; they differ
only in how they ensure that the increment operations are atomic. We need to
ensure atomicity because the numbers in the input sequence may appear multiple
times, and in different segments, making it possible for more than a single
thread to increment the same frequency counter at the same time.

The histogram benchmarks are performed with the following parameters: The full
input sequence consists of 4 million randomly chosen integers $a\in[0,31]$. The
sequence is split between 4, 8, and 48 threads for the i5, i7, and Xeon platform
respectively. The more threads, the less work per thread. 32 buckets are used,
one for each possible number in the input.

\subsubsection{Communication-free}
\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ms) & SD (\%) \\
		\hline
		\input{plots/histo-local-lockfree}
		\hline
		\hline
	\end{tabular}
	\caption{The histogram problem with no parallel communication. Time is
	the total wall-clock execution time, the standard deviation (SD) is
	given in percent of the execution time.}
	\label{table:histo-lockfree}
\end{figure}


\subsubsection{Coarse-grained locking}
The simplest solution is for the threads to take a single, shared lock anytime
they increment a frequency counter. This solution is slow. Only reading
from the input sequence is parallelised. At any time,
only one thread can be incrementing a counter.

\begin{code}
\begin{Verbatim}[frame=single]
  Object lock = new Object();
  let taskCount parallel tasks do {
    final int from = ..., to = ...;
    for(int j = from; j < to; ++j) {
      synchronized(lock){
        counters[inputSequence[j]].value++;
      }
    }
  }
\end{Verbatim}
	\caption{Simplified code for the threads in the coarse-grained locking
	version of the histogram problem.}
\end{code}

The table in figure \ref{table:histo-global} shows the execution times of this
solution to the histogram problem. At we would expect, it scales poorly with the number of cores.

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ms) & SD (\%) \\
		\hline
		\input{plots/histo-lock-global}
		\hline
		\hline
	\end{tabular}
	\caption{The histogram problem using a single global lock. Time is per
	input-element per thread, the standard deviation (SD) is given in
	percentage of the execution time.}
	\label{table:histo-global}
\end{figure}

We will not measure it, but there is a possibility for false sharing in this
implementation. Consider the following sequence of operations, where we assume
counter0 and counter1 are in the same cache line:

\begin{enumerate}
	\item CPU0 updates counter 0. The cache line is left in CPU0's cache.
	\item \label{list:invalidation} CPU1 updates counter 1. The cache line is invalidated in CPU0's
		cache.
	\item \label{list:unneededread} CPU0 updates counter 0. The counter is not in CPU0's cache, and
		must be read from CPU1's cache instead.
\end{enumerate}

The cache miss in step \ref{list:unneededread} is unnecessary. It only occurs
because the two counters share a cache line, and because the other counter was
updated in step \ref{list:invalidation} by a different CPU core. The impact of
false sharing here is likely overshadowed by the poor choice of locking scheme.

There is another kind of unnecessary/false communication between CPU cores here,
that has nothing to do with sharing of cache lines: When a thread takes the
lock, updates from previous threads are guaranteed to be made visible to it.
Except for updates to the counter the thread is \textit{trying} to increment,
this is unnecessary.

\subsubsection{Fine-grained locking}
A better solution is to have different counters guarded by different locks. This
way, threads can perform all their work in parallel, except when different
threads work on counters guarded by the same lock. For practical applications
the ideal level of granularity should be determined by experiment. In this
experiment I use 32 locks: One lock for each bucket.

\begin{code}
\begin{Verbatim}[frame=single]
  Object[] locks = ...;
  let taskCount parallel tasks do {
    final int from = ..., to = ...;
    for(int j = from; j < to; ++j) {
      int a = inputSequence[j];
      synchronized(locks[a]){
        counters[a].value++;
      }
    }
  }
\end{Verbatim}
\end{code}

The improved level of parallelism increases the possibility for false sharing: In
the previous version only one thread could perform writes at a time. In this
version, threads holding different locks can in theory spend arbitrary amounts
of time invalidating each other's cache lines without doing any actual work.

There are two clear candidates for false sharing: The locks and the counters.
Neither are stored directly in the arrays: Locks must be object types, and are
hence stored as references, and the integers used for the counters are stored in
placeholder objects so we can declare them as \java{volatile}. False sharing
of the actual array elements should therefore be irrelevant, but if the objects
are densely allocated, e.g. in a tight for-loop, they can still be placed
back-to-back in memory.

We measure the impact of false sharing by benchmarking versions with different
amounts of padding between the locks and counters.

\mynote{Because the elements/locks are objects, there is an additional,
unadvertised,  12 bytes of padding used for the counters, and something similar
for the locks (I can't say where in the object header the actual lock is, if it
is even in the object header). This should e noted somewhere, but probably not
here, as it goes for most of the benchmarks.}

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r r}
		\hline
		\hline
		Platform & Time wo. padding (ms) & Best time (ms) & Improvement \\
		\hline
		i5 & 113.0 & 100.88 & 10.7\% \\
		i7 & 71.88 & 59.9 & 16.9\% \\
		Xeon & 420.8 & 118.62 & 71.8\% \\
		\hline
		\hline
	\end{tabular}
	\caption{Best times vs. times without padding for the fine-grained
	histogram problem. Times are wall-clock execution times. Improvement is
	given in percent of the unpadded execution time: An improvement of
	71.8\% indicates that at least 71.8\% of the execution time is spend on
	unnecessary coherence communication in the unpadded version.}
	\label{table:hist-local}
\end{figure}

Figures \ref{fig:histo-local-i5}, \ref{fig:histo-local-i7}, and
\ref{fig:histo-local-xeon} show the wall-clock execution times using different
amounts of padding for the i5, i7, and Xeon platforms respectively. Each smaller
plot shows the execution time of the histogram problem, as a function of the
amount of padding between the counters. The third axis -- the amount of padding
between the locks -- is unrolled into different plots.

The table in figure \ref{table:hist-local} compares the unpadded times with the
best times for each platform.

The effect is smallest on the i5 platform: Using 128 bytes padding between
counters, and no padding between locks, yields a 10.7\% improvement on not using
padding. Padding the locks does not seem to benefit performance, and even makes
it worse in some cases.

On the i7 platform, padding locks and counters both benefits performance.
Padding 128 bytes between counters and 112 bytes between locks yields a 16.9\%
percent improvement on not using padding.

The most significant effect is seen on the Xeon platform. Like on the i7
platform, using padding is beneficial for both data structures, but here the
benefit from padding the locks is much more pronounced. Using 128 bytes of
padding for the counters and 112 bytes for the locks yields a 71.8\% improvement
on not using padding, indicating that most of the time is spent on cache
coherence overhead in the unpadded version.

There is a noteworthy caveat: The experiment runs with 48 threads on the Xeon
platform, but there are only 32 locks.
This cannot not be solved simply by increasing the
number of locks, as there are still only 32 distinct values in the input, and
therefore only 32 counters.
The scarcity of locks likely limits
throughput, but the false sharing impact should be the same without
this limitation, which is what we are concerned with.

It is not surprising that the platform with the most cores suffers the worst.
The more threads running in parallel, the bigger the likelihood of invalidating
a cache line that is relevant to another core. Furthermore, the invalidation
messages must be sent to all the other cores, and processors must wait for
acknowledgement messages from all the cores, so there is a lot more
communication occurring due the coherence protocol. The larger cache hierarchy
likely also means larger physical distances, which in turns makes communication
slower.

While our observations seem to agree with our understanding of false sharing,
two new mysteries present themselves: It is not clear why the execution time
keeps improving past 64-bytes padding, and it is not clear why padding the locks
has such a modest impact.

We would expect the performance to stop improving abruptly around 64
bytes, as that is the cache line size used by our hardware architectures.
One explanation for the continued improvements is the cache prefetch mechanism.
It is possible that the prefetch mechanism causes more than a single cache line
to be fetched. This is the explanation given for why the \java{@contended}
annotation causes 128 bytes of padding to inserted on openjdk, under the
assumption that cache lines are half that size\cite{openjdkmailcontended}.
This explanation is in agreement with the Intel optimization
manual\cite{inteloptimize}, which states that the spatial prefetcher
"strives to complete every cache line fetched to the L2 cache with the pair line
that completes it to a 128-byte aligned chunk"\footnotemark. While this
prefetching does not incur a false-sharing overhead, it will leave an additional
cache line in the CPU's cache. Another CPU might then later write to that
cache line, incurring unnecessary coherence overhead. The additional padding
does not prevent the prefetcher from loading the irrelevant cache line, but it
does prevent other threads from ever writing to it.

An explanation for the other mysterious artifact, the modest impact of padding
the locks, is more elusive. It seems to make sense that using a small number of
threads compared to the number of locks limits the impact of false sharing, as
it lessens the likelihood of threads working on the same cache lines. This
intuition fails to explain why the impact of padding the counters is larger: The
memory layout of the locks should be exactly the same as that of the counters,
and there are equally many of them.

\footnotetext{It is not entirely clear from the manual which microarchitectures
this applies to. It is stated for the Sandy Bridge microarchitecture, and seems
to apply to those that succeed it as well.}

\input{histo-plots.tex}

\subsubsection{Compare-and-swap}

\mynote{Figures
\ref{fig:histo-cas-i5},\ref{fig:histo-cas-i7},\ref{fig:histo-cas-xeon}}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/histo-cas-i5.tex}
\caption{The CAS histogram problem in the i5 platform. The plot shows the
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:histo-cas-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/histo-cas-i7.tex}
\caption{The CAS histogram problem in the i7 platform. The plot shows the
	wall-clock execution time in ns., as a function of padding in bytes.}
	\label{fig:histo-cas-i7}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/histo-cas-xeon.tex}
\caption{The CAS histogram problem in the Xeon platform. The plot shows the
	wall-clock execution time in ns., as a function of padding in bytes.}
	\label{fig:histo-cas-xeon}
\end{figure}

\subsection{Quicksort}
A famous example of a divide-and-conquer algorithm, quicksort recursively
divides the sorting problem into independent subproblems, combining partial
results into a solution for the whole problem. With respect to synchronization,
we can see this problem division as a kind of avoidance: If the sub-problems are
independent, they can be solved in parallel on a multicore system without any
communication - and therefore synchronization - between parallel processes.
Of coarse, we need to ensure that solutions are visible \textit{after} they are
found, but no communication is needed while the threads work.

\mynote{quicksort insert code snippet}

The program is a (somewhat naive) parallel implementation of quicksort. Anytime
the algorithm divides the problem in two, the calling thread forks a task to
solve the left part of the problem, executing the right part itself. Forked
tasks are run in parallel at the discretion of the Java \java{ForkJoinPool}.

As is common, and as is recommended in the popular algorithms text book by
\citet{sedgewick}, the implementation uses a cutoff to a sequential selection
sort. An experiment is included to find a good value for the cutoff.

There is no step to combine the results of two subproblems, as calls operate
in-place on the same shared array. While this may \textit{seem} like communication
between threads, there is only a single thread operating on a given
array-segment at any time. The only relevant communication is that of the
constructor parameters to the recursive calls, and the reference to the array
itself. Operations in
recursive calls are guaranteed to be visible to the caller because of the
call to \java{join()}.

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/qsort.tex}
	\caption{Quicksort on the i5 platform. The plot shows wall-clock execution time in
	ns., as a function of padding in bytes.}
	\label{fig:qsort-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/qsort-desktop.tex}
	\caption{Quicksort on the i7 platform. The plot shows wall-clock execution time in
	ns., as a function of padding in bytes.}
	\label{fig:qsort-i7}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/qsort-server.tex}
	\caption{Quicksort on the Xeon platform. The plot shows wall-clock execution time in
	ns., as a function of padding in bytes.}
	\label{fig:qsort-xeon}
\end{figure}

\mynote{include concrete parameters: list size(s), cutoff details}

\mynote{Analyze/conclude}

\subsection{K-means}

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ms) & SD (\%) \\
		\hline
		\input{plots/peterskmeans-i5}
		\hline
		\hline
	\end{tabular}
	\caption{The k-means problem on the i5 platform, using 4 tasks. Time is
	the total wall-clock execution time, the standard deviation (SD) is
	given as the percentage of the execution time.}
	\label{table:kmeans-i5}
\end{figure}

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ms) & SD (\%) \\
		\hline
		\input{plots/peterskmeans-i7}
		\hline
		\hline
	\end{tabular}
	\caption{The k-means problem on the i7 platform, using 8 tasks.  Time is
	the total wall-clock execution time, the standard deviation (SD) is
	given as the percentage of the execution time.}
	\label{table:kmeans-i7}
\end{figure}

\begin{figure}[hbtp]
	\centering
	\begin{tabular}{l r r}
		\hline
		\hline
		Platform & Time (ms) & SD (\%) \\
		\hline
		\input{plots/peterskmeans-xeon}
		\hline
		\hline
	\end{tabular}
	\caption{The k-means problem on the Xeon platform, using 48 tasks.
	Time is the total wall-clock execution time, the standard deviation (SD)
	is given as the percentage of the execution time.}
	\label{table:kmeans-xeon}
\end{figure}

\subsection{Striped Hashmap}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/hashmap-striped-i5.tex}
\caption{The striped hashmap problem on the i5 platform. The plot shows
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:hashmap-striped-i5}
\end{figure}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/hashmap-striped-i7-xeon.tex}
\caption{The striped hashmap problem on the i7 and Xeon platforms. The plot shows
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:hashmap-striped-i7}
\end{figure}

\subsection{Striped-write Hashmap}

\begin{figure}[hbpt]
\graphicspath{{plots/}}
\input{plots/hashmap-stripedwrite.tex}
\caption{The striped write-hashmap problem on all 3 platforms. The plot shows
	wall-clock execution time in ns., as a function of padding in bytes.}
\label{fig:hashmap-striped-xeon}
\end{figure}

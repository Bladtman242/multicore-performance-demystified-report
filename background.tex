\chapter{Background}
\mynote{introduce section ("previous work")}
In his paper \cite{mystery} from \citeyear{mystery}, \citeauthor{mystery} brings
to attention the curious effects of false sharing of CPU cache-lines on a Java
runtime platform. Using a k-means clustering implementation as example, he shows
how a seemingly obvious optimization (loop fusion) makes the program 70\% slower
in practice. That data-locality can affect performance is not surprising.
What is surprising is that while the sequential version of the program benefits
from the change, parallel versions suffer greatly. This is due to the effect
known as \textit{false sharing of cache lines} (or simply \textit{false
sharing}). He further finds that the effect of false sharing is highly
significant when locking on elements in an array, even though the individual
locks are uncontended. He subsequently shows that lock-striping implementations
of concurrent hashmaps can suffer greatly from false sharing.

While the difference in cache-behaviour between sequential and parallel programs
is confusing and unintuitive, it is not something new. In \citeyear{eggersbus},
\citeauthor{eggersbus} showed that CPU bus traffic as well as cache-miss
rates and trends differ between sequential (they use the term "uniprocessor")
programs and parallel programs \cite{eggersbus}.
They analyze cache behaviour for a small suite of programs, distinguishing
between applications with high and low per-processor data locality\footnote{They
define programs having high per-processor data locality as programs that perform
sequences of operations on a contiguous section of memory, where that section of
memory is not (or rarely) accessed by more than one processor at a given time.}.
They find that increasing the block (or cache line) size may improve
performance for sequential programs and parallel programs with high
per-processor data locality, but harms performance for parallel programs with
low per-processor data locality. This happens because the cost of additional cache
invalidations, and subsequent misses, outweighs the coherency overhead saved by
performing fewer, larger cache reads. They also find that increases in cache
size shifts the type of cache misses that occur in parallel programs: The larger
the cache, the more cache misses occur due to the coherency protocol (both in
absolute terms and relative to the total number of cache misses). The authors
suggest that the problem can be mitigated either by an optimizing compiler or
runtime environment, arranging shared data so that high per-processor data
locality is achieved.

Like the loop fusion optimization that introduced the problem in \cite{mystery},
it may seem that we need to significantly change our parallel algorithms to
avoid false sharing. However, it turns out that making simple transformations to
the data layout can go a long way. In \cite{mystery}, adding padding around
relevant variables significantly reduces the overhead.

In \citeyear{eggersReducing}, \citeauthor{eggersReducing} used static analysis
and compile-time code transformations to drastically reduce false sharing in a
small suite of coarse-grained\footnote{Coarse-grained here refers to the
granularity of of parallelism, not data sharing.} parallel programs
\cite{eggersReducing} (the suite used was not the same as in \cite{eggersbus}).
Using three different code transformations, they reduced false sharing by 64\%
on average, and by more than 90\% in the programs that were not
locality-optimized by the programmers beforehand. The benefits reaped in terms
of execution time varies greatly over the cache parameters of the platform, and
the number of processors used. For one program, the optimizations increased the
speedup gained from parallelism by a factor of $\sim 2.4$, yielding a $\sim 7$
times faster execution time than the sequential version, and $\sim 3$ times
faster than the unoptimized parallel version.

In \citeyear{TorrellasShared}, \citeauthor{TorrellasShared}
\cite{TorrellasShared} analyzed the effects of sharing (both true and false)
on a small suite of programs, before and after implementing a set of locality
optimizations. Curiously, they seem to expect that existing compilers already
pad around synchronization variables to prevent false sharing. As we shall see,
and as found in \cite{mystery}, this is not the case with Java. In fact, I am
not aware that this is the case for any particular compiler except for the work
produced in \cite{eggersReducing}

Their work \cite{TorrellasShared} shows that focusing on data sharing is
particularly important when using optimizing compilers. Not because optimizing
compilers make sharing worse (though they can), but because they are good at
eliminating memory accesses to processor-private data e.g. by keeping data in
CPU registers. Since they cannot do the same with shared data, as it would
circumvent the coherence ensured by the cache, memory access to shared data
often dominates IO traffic for parallel programs.
\citeauthor{TorrellasShared}  \cite{TorrellasShared}
provide two sets of optimizations: One requiring detailed profiling information
about the program to be optimized, and one that requires no such information. In
the authors' own words, their optimizations had a "small but significant
impact". Across their experiments, cache misses are reduced by 0.2-24\%. One
might speculate that the comparatively modest reductions are due to the small
cache line size used in their simulations (4-16 bytes vs. 4-256 bytes in
\cite{eggersReducing}, and 64 bytes in \cite{mystery}).

For the purposes of this report, false sharing is defined as unrelated data
being placed the same cache-coherence block, or cache line, as explained in
chapter \ref{chap:arch}. We gauge the impact of false sharing by hand-optimizing
programs to avoid it, and benchmarking the programs with and without those
optimizations. Measuring false sharing impact in this way will provide an
approximation at best. Unsatisfied with imprecision, some authors have tried to
bridge the gap between defining false sharing, and defining the impact of
false-sharing:

In \citeyear{falsedef}, \citeauthor{falsedef} \cite{falsedef} considered a handful
of definitions of false sharing, and conclude that none of them are satisfying.
They wish to find a definition that:

\begin{itemize}
	\item Agrees with intuition in that it has has numerical value
		corresponding to the cost savings attained by eliminating all
		false sharing. Hence it never grows as data is split over
		coherence blocks.
	\item Allows the properties of false sharing to be stated and proven as
		mathematical theorems, and
	\item Is practically measurable for real programs.
\end{itemize}

It seems unlikely that any definition of false sharing will satisfy their
criteria: Definitions can be based on comparing a program's
behaviour with that of an idealized version\footnotemark, or on analyzing
memory-operation sequences to assess unnecessary communication. In either case,
the definition must be able to distinguish the performance characteristics of false
sharing from those of all other communication in the memory hierarchy. Otherwise
it must accept oddities like negative costs of false sharing, as optimizations
to eliminate false sharing will often incur different IO overheads. No obvious
solution for distinguishing the performance impacts in the memory hierarchy
presents itself, and negative values for false sharing is unacceptable to
\citeauthor{falsedef}.

While a definition satisfying the above criteria would be helpful, we do not
need it in order to understand how false sharing occurs. We certainly
do not need it in order to avoid the costs associated with false sharing. In
fact, the "intuition" requirement directly contradicts our goal: Avoiding
performance pitfalls. That is, we wish to improve the execution time of our
programs where possible, so if an optimization increases execution time in spite
of reducing false sharing, we do not consider it an optimization.

The definition used in this report most closely resembles what
\citeauthor{falsedef} calls "the hand tuning method", which they attribute to
\citeauthor{eggersReducing} (referencing work not cited in this report).

\mynote{I haven't actually described bolosky and scotts methods yet}

\footnotetext{In their definitions, the authors alternate between considering
idealized hardware, avoiding false sharing by using small coherence-blocks, or
a policy -- software or hardware based -- that can place data ideally in the
memory hierarchy.}

\mynote{Write bg for the McKenney resources, and Drepper's what every programmer...}
\mynote{Write bg for the (not very hardware-oriented) resources used at the outset of the project}


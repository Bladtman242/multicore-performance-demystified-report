\chapter{Background}
\label{chap:background}
In this chapter we look over previous work that this report builds on. We first
look at work that addresses false sharing specifically, then we move on to works
describing multicore hardware and multicore software design in general.

In his paper from \citeyear{mystery} \cite{mystery}, \citeauthor{mystery} brings
to attention the curious effects of false sharing of cache-lines on a Java
runtime platform. Using a k-means clustering implementation as example, he shows
how a seemingly obvious optimization (loop fusion) makes the program 7.7 times
slower in practice. That data-locality can affect performance is not surprising.
What is surprising is that while the sequential version of the program benefits
from the change, parallel versions suffer greatly. This is due to the effect
known as \textit{false sharing of cache lines} (or simply \textit{false
sharing}). He further finds that the effect of false sharing is highly
significant when locking on elements in an array, even though the individual
locks are uncontended. He subsequently shows that lock-striping implementations
of concurrent hashmaps can suffer greatly from false sharing.

While the difference in cache-behaviour between sequential and multicore programs
is confusing and unintuitive, it is not something new. In \citeyear{eggersbus},
\citeauthor{eggersbus} showed that CPU bus traffic as well as cache-miss
rates and trends differ between sequential (they use the term "uniprocessor")
programs and multicore programs \cite{eggersbus}.
They analyze cache behaviour for a small suite of programs, distinguishing
between applications with high and low per-processor data locality\footnote{They
define programs having high per-processor data locality as programs that perform
sequences of operations on a contiguous section of memory, where that section of
memory is not (or rarely) accessed by more than one processor at a given time.}.
They find that increasing the block (or cache line) size may improve
performance for sequential programs and parallel programs with high
per-processor data locality, but harms performance for parallel programs with
low per-processor data locality. This happens because the cost of additional cache
invalidations, and subsequent misses, outweighs the coherency overhead saved by
performing fewer, larger cache reads. They also find that increases in cache
size shifts the type of cache misses that occur in parallel programs: The larger
the cache, the more cache misses occur due to the coherency protocol (both in
absolute terms and relative to the total number of cache misses). The authors
suggest that the problem can be mitigated either by an optimizing compiler or
runtime environment, arranging shared data so that high per-processor data
locality is achieved.

Like the loop fusion optimization that introduced the problem in \cite{mystery},
it may seem that we need to significantly change our parallel algorithms to
avoid false sharing. However, it turns out that making simple transformations to
the data layout can go a long way. In \cite{mystery}, adding padding around
relevant fields significantly reduces the overhead.

In \citeyear{eggersReducing}, \citeauthor{eggersReducing} used static analysis
and compile-time code transformations to drastically reduce false sharing in a
small suite of coarse-grained\footnote{Coarse-grained here refers to the
granularity of of parallelism, not data sharing.} parallel programs
\cite{eggersReducing} (the suite used was not the same as in \cite{eggersbus}).
Using three different code transformations, they reduced false sharing by 64\%
on average, and by more than 90\% in the programs that were not
locality-optimized by the programmers beforehand. The benefits reaped in terms
of execution time varies greatly over the cache parameters of the platform, and
the number of processors used. For one program, the optimizations increased the
speedup gained from parallelism by a factor of $\sim 2.4$, yielding a $\sim 7$
times faster execution time than the sequential version, and $\sim 3$ times
faster than the unoptimized parallel version.

In \citeyear{TorrellasShared}, \citeauthor{TorrellasShared}
\cite{TorrellasShared} analyzed the effects of sharing (both true and false)
on a small suite of programs, before and after implementing a set of locality
optimizations. Curiously, they seem to expect that existing compilers already
pad around synchronization variables to prevent false sharing. As we shall see,
and as found in \cite{mystery}, this is not the case with Java. In fact, I am
not aware that this is the case for any particular compiler except for the work
produced in \cite{eggersReducing}

Their work \cite{TorrellasShared} shows that focusing on data sharing is
particularly important when using optimizing compilers. Not because optimizing
compilers make sharing worse (though they can), but because they are good at
eliminating memory accesses to processor-private data e.g. by keeping data in
CPU registers. Since they cannot do the same with shared data, as it would
circumvent the coherence ensured by the cache, memory access to shared data
often dominates IO traffic for parallel programs.
\citeauthor{TorrellasShared}  \cite{TorrellasShared}
provide two sets of optimizations: One requiring detailed profiling information
about the program to be optimized, and one that requires no such information. In
the authors' own words, their optimizations had a "small but significant
impact". Across their experiments, cache misses are reduced by 0.2-24\%. One
might speculate that the comparatively modest reductions are due to the small
cache line size used in their simulations (4-16 bytes vs. 4-256 bytes in
\cite{eggersReducing}, and 64 bytes in \cite{mystery}).

For the purposes of this report, false sharing is defined as CPUs accessing
disjoint data sets placed the same cache-coherence block -- or cache line -- as
explained in chapter \ref{chap:arch}. We gauge the impact of false sharing by
hand-optimizing programs to avoid it, and benchmarking the programs with and
without those optimizations. Measuring false sharing impact in this way will
provide an approximation at best. Unsatisfied with this imprecision, some
authors have tried to bridge the gap between defining false sharing, and
defining the impact of false sharing:

In \citeyear{falsedef}, \citeauthor{falsedef} \cite{falsedef} considered a handful
of definitions of false sharing, and conclude that none of them are satisfying.
They wish to find a definition that:

\begin{itemize}
	\item Agrees with intuition in that it has has numerical value
		corresponding to the cost savings attained by eliminating all
		false sharing. Hence it never grows as data is split over
		coherence blocks.
	\item Allows the properties of false sharing to be stated and proven as
		mathematical theorems.
	\item Is practically measurable for real programs.
\end{itemize}

It seems unlikely that any definition of false sharing will satisfy their
criteria: Definitions can be based on comparing a program's
behaviour with that of an idealized version\footnotemark, or on analyzing
memory-operation sequences to assess unnecessary communication. In either case,
the definition must be able to distinguish the performance characteristics of false
sharing from those of all other communication in the memory hierarchy. Otherwise
it must accept oddities like negative costs of false sharing, as optimizations
to eliminate false sharing will often incur different IO overheads. No obvious
solution for distinguishing the performance impacts in the memory hierarchy
presents itself, and negative values for false sharing is unacceptable to
\citeauthor{falsedef}.

\footnotetext{In their definitions, the authors alternate between considering
idealized hardware, avoiding false sharing by using small coherence-blocks, or
a policy -- software or hardware based -- that can place data ideally in the
memory hierarchy.}

While a definition satisfying the above criteria would be helpful, we -- as
software designers -- do not need it in order to understand how false sharing
occurs. We certainly do not need it in order to avoid the costs associated with
false sharing. In fact, the "intuition" requirement directly contradicts our
goal: Avoiding performance pitfalls. That is, we wish to improve the execution
time of our programs where possible, so if an optimization increases execution
time in spite of reducing false sharing, we do not consider it an optimization.


In his paper from \citeyear{mckenny-barriers} \cite{mckenny-barriers}, \citeauthor{mckenny-barriers}
describes cache coherence behaviours and memory barriers using detailed
examples. The focus is on the MESI cache coherence protocol, initially
presented, but apparently not named, by \citeauthor{mesi} \cite{mesi}.
\citeauthor{mckenny-barriers}'s paper
does not explicitly deal with false sharing, but lays the foundation for
understanding it from a hardware perspective. In his book, \textit{Is Parallel
Programming Hard, And, If So, What Can You Do About It?}\cite{mckenney},
\citeauthor{mckenney} goes into much more detail with the design of CPUs as well
as the memory hierarchy. Unlike the \citeyear{mckenny-barriers} paper, the
focus of the book is software design, and several design patterns for multicore
software are discussed, in addition to formal verification methods for multicore
software. The book mentions and describes false sharing, but for the most part
assumes the reader will know how to avoid it (the target audience appears to
be software designers experienced with sequential C programming).

In \textit{What Every Programmer Should Know About Memory}
\cite{whatprogrammersshouldknow}, \citeauthor{whatprogrammersshouldknow} focuses
on both modern and historical designs used in the memory hierarchy.
\citeauthor{whatprogrammersshouldknow} describes different types of caches, in
enough detail to understand the performance characteristics of false sharing of
cache lines, as well as other unfortunate memory access patterns.

An authoritative work on hardware architecture, \textit{Computer Architecture:
A Quantitative Approach} \cite{quantarch} describes false sharing. The fourth,
fifth and sixth editions go a little further into analysing false sharing
overhead, but the book does not go into detail on how to avoid it.

\textit{Concurrent Programming in Java: Design Principles and Patterns} by
\citeauthor{lea} describes multicore programming in Java. \citeauthor{lea}
describes the Java concurrency primitives available to the programmer, and the
visibility and ordering effects that are guaranteed by the Java language
specification. He briefly describes how object oriented design differs when
writing sequential vs. multicore programs, and suggests several patterns for
thread safe object oriented design.

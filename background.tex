\chapter{Background}
\mynote{introduce section ("previous work")}
In his paper \cite{mystery} from \citeyear{mystery}, \citeauthor{mystery} brings to attention the curious effects of
false sharing of CPU cache-lines on a Java runtime platform. Using a k-means
clustering implementation as example, he shows how a seemingly obvious
optimization (loop fusion) makes the program 70\% slower in practice. That loop
transformations can affect data-locality\mynote{this might need a reference},
and thereby cache utilization, is not surprising. What is surprising is that
while the sequential version of the program benefits from the change, parallel
versions suffer greatly. This is due to the effect known as \textit{false
sharing of cache lines} (or simply \textit{false sharing}). He further finds that
the effect of false sharing is highly significant when locking on elements in an
array, even though the individual locks are uncontended. He subsequently shows that
lock-striping implementations of concurrent hashmaps can suffer greatly from
false sharing.

While the variance in cache-behaviour between sequential and parallel programs
is confusing and unintuitive, it is not something new. In \citeyear{Eggersbus},
\citeauthor{Eggersbus} showed that cpu bus consumption as well as cache-miss
rates and trends differ between sequential (they use the term "uniprocessor")
programs and parallel programs \cite{Eggersbus}.
They analyze cache behaviour for a small suite of programs, distinguishing
between applications with high and low per-processor data locality\footnote{They
define programs having high per-processor data locality as programs that perform
sequences of operations on a contiguous section of memory, where that section of
memory is not (or rarely) accessed by more than one processor at a given time}.
They find that increasing the block (or cache line) size may improve
performance for sequential programs and parallel programs with high
per-processor data locality, but harms performance for parallel programs with
low per-processor data locality. This happens when the cost of additional cache
invalidations, and subsequent misses, outweighs the coherency overhead saved by
performing fewer, larger cache reads. They also find that increases in cache
size shifts the type of cache misses that occur in parallel programs: The larger
the cache, the more cache misses occur due to the coherency protocol (both in
absolute terms and relative to the total number of cache misses). The authors
suggest that the problem can be mitigated either by an optimizing compiler or
runtime environment, arranging shared data so that high per-processor data
locality is achieved.

Like the loop fusion optimization that introduced the problem in \cite{mystery},
it may seem that we need to significantly change our parallel algorithms to
avoid false sharing. However, it turns out that making simple transformations to
the data layout can go a long way. In \cite{mystery}, adding padding around
relevant variables significantly reduces the overhead.

In \citeyear{EggersReducing}, \citeauthor{EggersReducing} used static analysis
and compile-time code transformations to drastically reduce false sharing in a
small suite of course-grained\footnote{Course-grained here refers to the
granularity of of parallelism, not data sharing} parallel programs
\cite{EggersReducing} (the suite used was not the same as in \cite{Eggersbus}).
Using three different code transformations, they reduced false sharing by 64\%
on average, and by more than 90\% in the programs that were not
locality-optimized by the programmers beforehand. The benefits reaped in terms
of execution time varies greatly over the cache parameters of the platform, and
the number of processors used. For one program, the optimizations increased the
speedup gained from parallelism by a factor of $\sim 2.4$, yielding a $\sim 7$
times faster execution time than the sequential version, and $\sim 3$ times
faster than the unoptimized parallel version.

In \citeyear{TorrellasShared}, \citeauthor{TorrellasShared}
\cite{TorrellasShared} analyzed the effects of sharing (both true and false)
on a small suite of programs, before and after implementing a set of locality
optimizations. Curiously, they seem to expect that existing compilers already
pad around synchronization variables to prevent false sharing. As we shall see,
and as found in \cite{mystery}, this is not the case with Java. In fact, I am
not aware that this is the case for any particular compiler except for the work
produced in \cite{EggersReducing} \mynote{time permitting, I'll test this for
gcc}.
The work in \cite{TorrellasShared} shows that focusing on data sharing is particularly important when
using optimizing compilers. This is not necessarily because optimizing compilers
make sharing worse, but because they are good at eliminating memory accesses to
processor-private data e.g. by keeping data in CPU registers. This wouldn't work
for shared data, as it would circumvent the coherence ensured by the cache.
Hence, when using an optimizing compiler, memory access to shared data often
dominates IO utilization for parallel programs. They provide two sets of
optimizations: One requiring detailed profiling information about the program to
be optimized, and one that requires no such information. In the authors' own
words, their optimizations had a "small but significant impact". Across their
experiments, cache misses are reduced by 0.2-24\%. One might speculate that the
comparatively modest reductions are due to the small cache line size used in
their simulations (4-16 bytes vs. 4-256 bytes in \cite{EggersReducing}, and 64
bytes in \cite{mystery}).

In \citeyear{falsedef},
\citeauthor{falsedef} \cite{falsedef}

\mynote{Write background for Bolosky\&Scott}

\mynote{Write bg for the McKenney resources, and Drepper's what every programmer...}
\mynote{Write bg for the (not very hardware-specific) resources used at the outset of the project}

